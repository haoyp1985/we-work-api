# ğŸ“ˆ æ‰©å±•æ€§è¯¦ç»†è®¾è®¡
*WeWork Management Platform - Scalability Design*

## ğŸ“‹ æ‰©å±•æ€§è®¾è®¡æ¦‚è§ˆ

### ğŸ¯ æ‰©å±•æ€§ç›®æ ‡
- **æ°´å¹³æ‰©å±•**: æ”¯æŒçº¿æ€§æ‰©å±•åˆ°1000+èŠ‚ç‚¹
- **å¼¹æ€§ä¼¸ç¼©**: è‡ªåŠ¨æ ¹æ®è´Ÿè½½è°ƒæ•´èµ„æº
- **é«˜å¯ç”¨æ€§**: å¤šåœ°åŸŸéƒ¨ç½²ï¼Œå¯ç”¨æ€§ > 99.95%
- **æ€§èƒ½ä¿è¯**: æ‰©å±•è¿‡ç¨‹ä¸­æ€§èƒ½ä¸é™çº§
- **æˆæœ¬ä¼˜åŒ–**: æ™ºèƒ½èµ„æºåˆ†é…ï¼Œé™ä½è¿è¥æˆæœ¬

### ğŸ—ï¸ æ‰©å±•æ€§æ¶æ„è®¾è®¡

```mermaid
graph TB
    subgraph "è´Ÿè½½å‡è¡¡å±‚"
        LB1[å…¨å±€è´Ÿè½½å‡è¡¡]
        LB2[åŒºåŸŸè´Ÿè½½å‡è¡¡]
        LB3[æœåŠ¡è´Ÿè½½å‡è¡¡]
    end
    
    subgraph "åº”ç”¨å±‚æ‰©å±•"
        AS1[APIç½‘å…³é›†ç¾¤]
        AS2[å¾®æœåŠ¡é›†ç¾¤]
        AS3[æ¶ˆæ¯é˜Ÿåˆ—é›†ç¾¤]
        AS4[ç¼“å­˜é›†ç¾¤]
    end
    
    subgraph "æ•°æ®å±‚æ‰©å±•"
        DS1[æ•°æ®åº“é›†ç¾¤]
        DS2[åˆ†å¸ƒå¼å­˜å‚¨]
        DS3[æœç´¢å¼•æ“é›†ç¾¤]
        DS4[æ—¶åºæ•°æ®åº“]
    end
    
    subgraph "åŸºç¡€è®¾æ–½å±‚"
        IS1[å®¹å™¨ç¼–æ’]
        IS2[æœåŠ¡ç½‘æ ¼]
        IS3[ç›‘æ§ç³»ç»Ÿ]
        IS4[æ—¥å¿—ç³»ç»Ÿ]
    end
    
    subgraph "æ‰©ç¼©å®¹æ§åˆ¶"
        SC1[HPAæ§åˆ¶å™¨]
        SC2[VPAæ§åˆ¶å™¨]
        SC3[è‡ªå®šä¹‰æ§åˆ¶å™¨]
        SC4[é¢„æµ‹å¼•æ“]
    end
    
    LB1 --> AS1
    AS1 --> AS2
    AS2 --> DS1
    IS1 --> SC1
    SC1 --> AS2
```

## ğŸ”§ å¾®æœåŠ¡æ‰©å±•æ€§è®¾è®¡

### æœåŠ¡æ‹†åˆ†å’Œç»„åˆç­–ç•¥
```java
@Component
public class ServiceScalabilityManager {
    
    private final ServiceRegistry serviceRegistry;
    private final LoadBalancer loadBalancer;
    private final ServiceMesh serviceMesh;
    
    // åŠ¨æ€æœåŠ¡å‘ç°å’Œæ³¨å†Œ
    public void registerService(ServiceInstance instance) {
        // å¥åº·æ£€æŸ¥
        if (performHealthCheck(instance)) {
            serviceRegistry.register(instance);
            loadBalancer.addInstance(instance);
            
            // æ›´æ–°æœåŠ¡ç½‘æ ¼é…ç½®
            serviceMesh.updateRouting(instance.getServiceName(), 
                serviceRegistry.getInstances(instance.getServiceName()));
            
            log.info("Service instance registered: {}", instance);
        }
    }
    
    // æ™ºèƒ½æœåŠ¡åˆ†ç‰‡
    public void enableServiceSharding(String serviceName, ShardingStrategy strategy) {
        List<ServiceInstance> instances = serviceRegistry.getInstances(serviceName);
        
        switch (strategy.getType()) {
            case TENANT_BASED:
                enableTenantBasedSharding(serviceName, instances);
                break;
            case REGION_BASED:
                enableRegionBasedSharding(serviceName, instances);
                break;
            case FEATURE_BASED:
                enableFeatureBasedSharding(serviceName, instances);
                break;
        }
    }
    
    private void enableTenantBasedSharding(String serviceName, List<ServiceInstance> instances) {
        // æŒ‰ç§Ÿæˆ·åˆ†ç‰‡ï¼Œæ¯ä¸ªç§Ÿæˆ·è·¯ç”±åˆ°ç‰¹å®šçš„æœåŠ¡å®ä¾‹
        Map<String, List<ServiceInstance>> tenantShards = groupInstancesByTenant(instances);
        
        for (Map.Entry<String, List<ServiceInstance>> entry : tenantShards.entrySet()) {
            String tenantId = entry.getKey();
            List<ServiceInstance> tenantInstances = entry.getValue();
            
            // é…ç½®è·¯ç”±è§„åˆ™
            RoutingRule rule = RoutingRule.builder()
                .serviceName(serviceName)
                .condition("headers['tenant-id'] == '" + tenantId + "'")
                .targets(tenantInstances)
                .build();
            
            serviceMesh.addRoutingRule(rule);
        }
    }
    
    // æœåŠ¡å®ä¾‹é¢„çƒ­
    public void warmupServiceInstance(ServiceInstance instance) {
        CompletableFuture.runAsync(() -> {
            try {
                // é¢„åŠ è½½ç¼“å­˜
                preloadCache(instance);
                
                // é¢„çƒ­JVM
                warmupJVM(instance);
                
                // é¢„è¿æ¥æ•°æ®åº“
                preconnectDatabase(instance);
                
                // æ ‡è®°ä¸ºå¯ç”¨
                markInstanceReady(instance);
                
            } catch (Exception e) {
                log.error("Failed to warmup service instance: " + instance, e);
            }
        });
    }
    
    private void preloadCache(ServiceInstance instance) {
        // é¢„åŠ è½½å¸¸ç”¨æ•°æ®åˆ°ç¼“å­˜
        List<String> hotDataKeys = getHotDataKeys(instance.getServiceName());
        
        for (String key : hotDataKeys) {
            try {
                cacheService.preload(key);
            } catch (Exception e) {
                log.warn("Failed to preload cache key: " + key, e);
            }
        }
    }
    
    private void warmupJVM(ServiceInstance instance) {
        // å‘é€é¢„çƒ­è¯·æ±‚ï¼Œè§¦å‘JITç¼–è¯‘
        List<String> warmupEndpoints = getWarmupEndpoints(instance.getServiceName());
        
        for (String endpoint : warmupEndpoints) {
            for (int i = 0; i < 100; i++) {
                try {
                    httpClient.get(instance.getUrl() + endpoint).execute();
                } catch (Exception e) {
                    // å¿½ç•¥é¢„çƒ­è¯·æ±‚çš„é”™è¯¯
                }
            }
        }
    }
}

// æ™ºèƒ½è´Ÿè½½å‡è¡¡
@Component
public class IntelligentLoadBalancer {
    
    private final Map<String, LoadBalancingStrategy> strategies = new HashMap<>();
    private final PerformanceMonitor performanceMonitor;
    
    @PostConstruct
    public void initializeStrategies() {
        strategies.put("round_robin", new RoundRobinStrategy());
        strategies.put("weighted_round_robin", new WeightedRoundRobinStrategy());
        strategies.put("least_connections", new LeastConnectionsStrategy());
        strategies.put("response_time", new ResponseTimeBasedStrategy());
        strategies.put("adaptive", new AdaptiveStrategy());
    }
    
    public ServiceInstance selectInstance(String serviceName, LoadBalancingRequest request) {
        List<ServiceInstance> availableInstances = getAvailableInstances(serviceName);
        
        if (availableInstances.isEmpty()) {
            throw new NoAvailableInstanceException("No available instances for service: " + serviceName);
        }
        
        LoadBalancingStrategy strategy = determineStrategy(serviceName, request);
        return strategy.select(availableInstances, request);
    }
    
    private LoadBalancingStrategy determineStrategy(String serviceName, LoadBalancingRequest request) {
        // æ ¹æ®æœåŠ¡ç‰¹æ€§å’Œè¯·æ±‚ç‰¹æ€§é€‰æ‹©ç­–ç•¥
        if (request.isLatencySensitive()) {
            return strategies.get("response_time");
        }
        
        if (request.isStateful()) {
            return strategies.get("least_connections");
        }
        
        // é»˜è®¤ä½¿ç”¨è‡ªé€‚åº”ç­–ç•¥
        return strategies.get("adaptive");
    }
}

// è‡ªé€‚åº”è´Ÿè½½å‡è¡¡ç­–ç•¥
public class AdaptiveStrategy implements LoadBalancingStrategy {
    
    private final PerformanceMonitor performanceMonitor;
    
    @Override
    public ServiceInstance select(List<ServiceInstance> instances, LoadBalancingRequest request) {
        Map<ServiceInstance, Double> scores = new HashMap<>();
        
        for (ServiceInstance instance : instances) {
            double score = calculateInstanceScore(instance);
            scores.put(instance, score);
        }
        
        // é€‰æ‹©å¾—åˆ†æœ€é«˜çš„å®ä¾‹
        return scores.entrySet().stream()
            .max(Map.Entry.comparingByValue())
            .map(Map.Entry::getKey)
            .orElse(instances.get(0));
    }
    
    private double calculateInstanceScore(ServiceInstance instance) {
        PerformanceMetrics metrics = performanceMonitor.getMetrics(instance.getId());
        
        // ç»¼åˆè€ƒè™‘å¤šä¸ªå› ç´ 
        double cpuScore = 1.0 - metrics.getCpuUsage();           // CPUä½¿ç”¨ç‡è¶Šä½è¶Šå¥½
        double memoryScore = 1.0 - metrics.getMemoryUsage();     // å†…å­˜ä½¿ç”¨ç‡è¶Šä½è¶Šå¥½
        double responseScore = 1.0 / metrics.getAverageResponseTime(); // å“åº”æ—¶é—´è¶ŠçŸ­è¶Šå¥½
        double errorScore = 1.0 - metrics.getErrorRate();        // é”™è¯¯ç‡è¶Šä½è¶Šå¥½
        double connectionScore = 1.0 - (metrics.getActiveConnections() / metrics.getMaxConnections());
        
        // åŠ æƒè®¡ç®—æ€»åˆ†
        return (cpuScore * 0.2 + 
                memoryScore * 0.2 + 
                responseScore * 0.3 + 
                errorScore * 0.2 + 
                connectionScore * 0.1);
    }
}
```

### æœåŠ¡ç½‘æ ¼é›†æˆ
```yaml
# IstioæœåŠ¡ç½‘æ ¼é…ç½®
apiVersion: networking.istio.io/v1beta1
kind: VirtualService
metadata:
  name: wework-api-virtual-service
spec:
  hosts:
  - wework-api
  http:
  - match:
    - headers:
        tenant-id:
          exact: "tenant-a"
    route:
    - destination:
        host: wework-api
        subset: tenant-a
      weight: 100
  - match:
    - headers:
        tenant-id:
          exact: "tenant-b"
    route:
    - destination:
        host: wework-api
        subset: tenant-b
      weight: 100
  - route:
    - destination:
        host: wework-api
        subset: default
      weight: 100

---
apiVersion: networking.istio.io/v1beta1
kind: DestinationRule
metadata:
  name: wework-api-destination-rule
spec:
  host: wework-api
  trafficPolicy:
    connectionPool:
      tcp:
        maxConnections: 100
      http:
        http1MaxPendingRequests: 100
        maxRequestsPerConnection: 10
    loadBalancer:
      consistentHash:
        httpHeaderName: "tenant-id"
  subsets:
  - name: tenant-a
    labels:
      tenant: tenant-a
    trafficPolicy:
      connectionPool:
        tcp:
          maxConnections: 50
  - name: tenant-b
    labels:
      tenant: tenant-b
    trafficPolicy:
      connectionPool:
        tcp:
          maxConnections: 50
  - name: default
    labels:
      version: v1
```

## ğŸ—„ï¸ æ•°æ®å­˜å‚¨æ‰©å±•æ€§

### åˆ†å¸ƒå¼æ•°æ®åº“é›†ç¾¤
```java
@Configuration
public class DistributedDatabaseConfiguration {
    
    // åˆ†ç‰‡æ•°æ®æºé…ç½®
    @Bean
    public DataSource createShardingDataSource() {
        // 4ä¸ªä¸»åº“ï¼Œæ¯ä¸ªä¸»åº“2ä¸ªä»åº“
        Map<String, DataSource> dataSourceMap = createDataSourceMap();
        
        // åˆ†ç‰‡è§„åˆ™
        ShardingRuleConfiguration shardingRule = createShardingRules();
        
        // è¯»å†™åˆ†ç¦»è§„åˆ™
        ReadWriteSplitRuleConfiguration readWriteSplitRule = createReadWriteSplitRules();
        
        // å¹¿æ’­è¡¨é…ç½®
        shardingRule.getBroadcastTables().addAll(Arrays.asList(
            "sys_configs", "sys_dictionaries", "sys_permissions"
        ));
        
        Properties properties = new Properties();
        properties.setProperty("sql.show", "false");
        properties.setProperty("check.table.metadata.enabled", "false");
        
        return ShardingDataSourceFactory.createDataSource(
            dataSourceMap, 
            Arrays.asList(shardingRule, readWriteSplitRule), 
            properties
        );
    }
    
    private Map<String, DataSource> createDataSourceMap() {
        Map<String, DataSource> dataSourceMap = new HashMap<>();
        
        // åˆ›å»ºåˆ†ç‰‡æ•°æ®æº
        for (int i = 0; i < 4; i++) {
            // ä¸»åº“
            HikariDataSource masterDs = createMasterDataSource(i);
            dataSourceMap.put("ds" + i + "_master", masterDs);
            
            // ä»åº“
            for (int j = 0; j < 2; j++) {
                HikariDataSource slaveDs = createSlaveDataSource(i, j);
                dataSourceMap.put("ds" + i + "_slave" + j, slaveDs);
            }
        }
        
        return dataSourceMap;
    }
    
    private ShardingRuleConfiguration createShardingRules() {
        ShardingRuleConfiguration config = new ShardingRuleConfiguration();
        
        // ç”¨æˆ·è¡¨åˆ†ç‰‡
        TableRuleConfiguration userTableRule = new TableRuleConfiguration("users");
        userTableRule.setActualDataNodes("ds${0..3}_master.users_${0..15}");
        userTableRule.setDatabaseShardingStrategyConfig(
            new StandardShardingStrategyConfiguration("user_id", new UserDatabaseShardingAlgorithm())
        );
        userTableRule.setTableShardingStrategyConfig(
            new StandardShardingStrategyConfiguration("user_id", new UserTableShardingAlgorithm())
        );
        userTableRule.setKeyGenerateStrategyConfig(
            new KeyGenerateStrategyConfiguration("user_id", "snowflake")
        );
        
        // æ¶ˆæ¯è¡¨åˆ†ç‰‡
        TableRuleConfiguration messageTableRule = new TableRuleConfiguration("message_records");
        messageTableRule.setActualDataNodes("ds${0..3}_master.message_records_${202401..202412}");
        messageTableRule.setDatabaseShardingStrategyConfig(
            new StandardShardingStrategyConfiguration("tenant_id", new TenantDatabaseShardingAlgorithm())
        );
        messageTableRule.setTableShardingStrategyConfig(
            new StandardShardingStrategyConfiguration("created_at", new DateTableShardingAlgorithm())
        );
        
        config.getTableRuleConfigs().addAll(Arrays.asList(userTableRule, messageTableRule));
        
        return config;
    }
}

// åŠ¨æ€åˆ†ç‰‡æ‰©å®¹
@Component
public class ShardingExpansionService {
    
    private final ShardingDataSource shardingDataSource;
    private final MetadataManager metadataManager;
    
    // åœ¨çº¿æ‰©å®¹åˆ†ç‰‡
    public void expandShards(String tableName, int newShardCount) {
        try {
            // 1. åˆ›å»ºæ–°çš„åˆ†ç‰‡è¡¨
            createNewShardTables(tableName, newShardCount);
            
            // 2. æ›´æ–°åˆ†ç‰‡è§„åˆ™
            updateShardingRules(tableName, newShardCount);
            
            // 3. æ•°æ®è¿ç§»
            migrateDataToNewShards(tableName, newShardCount);
            
            // 4. éªŒè¯æ•°æ®ä¸€è‡´æ€§
            validateDataConsistency(tableName);
            
            // 5. åˆ‡æ¢æµé‡
            switchTrafficToNewShards(tableName);
            
            log.info("Successfully expanded shards for table: {}", tableName);
            
        } catch (Exception e) {
            log.error("Failed to expand shards for table: " + tableName, e);
            rollbackExpansion(tableName);
            throw e;
        }
    }
    
    private void migrateDataToNewShards(String tableName, int newShardCount) {
        String migrationJobId = "migration_" + tableName + "_" + System.currentTimeMillis();
        
        // åˆ›å»ºæ•°æ®è¿ç§»ä»»åŠ¡
        DataMigrationJob job = DataMigrationJob.builder()
            .jobId(migrationJobId)
            .sourceTable(tableName)
            .newShardCount(newShardCount)
            .batchSize(1000)
            .parallelism(4)
            .build();
        
        dataMigrationService.submitJob(job);
        
        // ç›‘æ§è¿ç§»è¿›åº¦
        while (!job.isCompleted()) {
            DataMigrationProgress progress = job.getProgress();
            log.info("Migration progress: {}%, migrated: {}, total: {}", 
                    progress.getPercentage(), progress.getMigrated(), progress.getTotal());
            
            try {
                Thread.sleep(10000); // æ¯10ç§’æ£€æŸ¥ä¸€æ¬¡
            } catch (InterruptedException e) {
                Thread.currentThread().interrupt();
                break;
            }
        }
        
        if (job.isFailed()) {
            throw new DataMigrationException("Data migration failed: " + job.getFailureReason());
        }
    }
}
```

### æ•°æ®åº“è¿æ¥æ± æ‰©å±•
```java
@Component
public class DynamicDataSourceManager {
    
    private final Map<String, HikariDataSource> dataSources = new ConcurrentHashMap<>();
    private final DataSourceConfig config;
    
    // åŠ¨æ€åˆ›å»ºæ•°æ®æº
    public DataSource getOrCreateDataSource(String dataSourceKey) {
        return dataSources.computeIfAbsent(dataSourceKey, key -> {
            HikariConfig hikariConfig = new HikariConfig();
            
            // ä»é…ç½®ä¸­å¿ƒè·å–æ•°æ®åº“è¿æ¥ä¿¡æ¯
            DatabaseConnectionInfo connInfo = config.getConnectionInfo(key);
            
            hikariConfig.setJdbcUrl(connInfo.getJdbcUrl());
            hikariConfig.setUsername(connInfo.getUsername());
            hikariConfig.setPassword(connInfo.getPassword());
            
            // åŠ¨æ€è°ƒæ•´è¿æ¥æ± å¤§å°
            int poolSize = calculateOptimalPoolSize(key);
            hikariConfig.setMaximumPoolSize(poolSize);
            hikariConfig.setMinimumIdle(poolSize / 4);
            
            // è¿æ¥æ± ç›‘æ§
            hikariConfig.setRegisterMbeans(true);
            hikariConfig.setPoolName(key + "-pool");
            
            // è¿æ¥æœ‰æ•ˆæ€§æ£€æŸ¥
            hikariConfig.setConnectionTestQuery("SELECT 1");
            hikariConfig.setValidationTimeout(5000);
            
            HikariDataSource dataSource = new HikariDataSource(hikariConfig);
            
            // æ³¨å†Œæ•°æ®æºç›‘æ§
            registerDataSourceMonitoring(key, dataSource);
            
            return dataSource;
        });
    }
    
    private int calculateOptimalPoolSize(String dataSourceKey) {
        // åŸºäºå†å²è´Ÿè½½æ•°æ®è®¡ç®—æœ€ä½³è¿æ¥æ± å¤§å°
        ConnectionUsageStats stats = connectionMonitor.getUsageStats(dataSourceKey);
        
        if (stats == null) {
            return 20; // é»˜è®¤å¤§å°
        }
        
        double avgUsage = stats.getAverageUsage();
        double peakUsage = stats.getPeakUsage();
        
        // åŸºäºå³°å€¼ä½¿ç”¨é‡çš„120%è®¾ç½®æ± å¤§å°
        int calculatedSize = (int) Math.ceil(peakUsage * 1.2);
        
        // é™åˆ¶åœ¨åˆç†èŒƒå›´å†…
        return Math.max(10, Math.min(100, calculatedSize));
    }
    
    // åŠ¨æ€è°ƒæ•´è¿æ¥æ± 
    @Scheduled(fixedRate = 300000) // æ¯5åˆ†é’Ÿæ‰§è¡Œä¸€æ¬¡
    public void adjustConnectionPools() {
        for (Map.Entry<String, HikariDataSource> entry : dataSources.entrySet()) {
            String key = entry.getKey();
            HikariDataSource dataSource = entry.getValue();
            
            int currentSize = dataSource.getMaximumPoolSize();
            int optimalSize = calculateOptimalPoolSize(key);
            
            if (Math.abs(currentSize - optimalSize) > 5) {
                log.info("Adjusting connection pool size for {}: {} -> {}", 
                        key, currentSize, optimalSize);
                
                dataSource.setMaximumPoolSize(optimalSize);
                dataSource.setMinimumIdle(optimalSize / 4);
            }
        }
    }
    
    // æ•°æ®æºå¥åº·æ£€æŸ¥
    @Scheduled(fixedRate = 60000) // æ¯åˆ†é’Ÿæ‰§è¡Œä¸€æ¬¡
    public void performHealthCheck() {
        for (Map.Entry<String, HikariDataSource> entry : dataSources.entrySet()) {
            String key = entry.getKey();
            HikariDataSource dataSource = entry.getValue();
            
            try {
                // æ£€æŸ¥è¿æ¥æ± çŠ¶æ€
                HikariPoolMXBean poolMXBean = dataSource.getHikariPoolMXBean();
                
                if (poolMXBean.getActiveConnections() > poolMXBean.getMaxPoolSize() * 0.9) {
                    alertService.sendConnectionPoolAlert(key, "High connection usage");
                }
                
                // æµ‹è¯•è¿æ¥
                try (Connection conn = dataSource.getConnection()) {
                    conn.isValid(5);
                }
                
            } catch (Exception e) {
                log.error("Health check failed for data source: " + key, e);
                alertService.sendDataSourceAlert(key, "Health check failed");
            }
        }
    }
}
```

## ğŸš€ ç¼“å­˜æ‰©å±•æ€§è®¾è®¡

### åˆ†å¸ƒå¼ç¼“å­˜é›†ç¾¤
```java
@Configuration
public class DistributedCacheConfiguration {
    
    @Bean
    public RedisClusterConfiguration redisClusterConfiguration() {
        RedisClusterConfiguration config = new RedisClusterConfiguration();
        
        // é›†ç¾¤èŠ‚ç‚¹é…ç½®
        Set<RedisNode> nodes = new HashSet<>();
        for (int i = 0; i < 6; i++) {
            nodes.add(new RedisNode("redis-cluster-" + i, 6379));
        }
        config.setClusterNodes(nodes);
        
        // é›†ç¾¤é…ç½®
        config.setMaxRedirects(3);
        
        return config;
    }
    
    @Bean
    public LettuceConnectionFactory redisConnectionFactory() {
        LettuceClientConfiguration clientConfig = LettuceClientConfiguration.builder()
            .commandTimeout(Duration.ofSeconds(5))
            .shutdownTimeout(Duration.ofSeconds(10))
            .clientResources(DefaultClientResources.builder()
                .ioThreadPoolSize(8)
                .computationThreadPoolSize(8)
                .build())
            .build();
        
        return new LettuceConnectionFactory(redisClusterConfiguration(), clientConfig);
    }
    
    @Bean
    public RedisTemplate<String, Object> redisTemplate() {
        RedisTemplate<String, Object> template = new RedisTemplate<>();
        template.setConnectionFactory(redisConnectionFactory());
        
        // ä½¿ç”¨ä¸€è‡´æ€§å“ˆå¸Œåºåˆ—åŒ–å™¨
        template.setKeySerializer(new StringRedisSerializer());
        template.setValueSerializer(new GenericJackson2JsonRedisSerializer());
        template.setHashKeySerializer(new StringRedisSerializer());
        template.setHashValueSerializer(new GenericJackson2JsonRedisSerializer());
        
        return template;
    }
}

// ç¼“å­˜åˆ†ç‰‡ç®¡ç†
@Component
public class CacheShardingManager {
    
    private final ConsistentHash<CacheNode> consistentHash;
    private final Set<CacheNode> cacheNodes;
    private final ReplicationFactor replicationFactor = new ReplicationFactor(3);
    
    public CacheShardingManager() {
        this.cacheNodes = initializeCacheNodes();
        this.consistentHash = new ConsistentHash<>(cacheNodes, 160); // 160ä¸ªè™šæ‹ŸèŠ‚ç‚¹
    }
    
    // è·å–ç¼“å­˜èŠ‚ç‚¹
    public List<CacheNode> getCacheNodes(String key) {
        // ä¸»èŠ‚ç‚¹
        CacheNode primaryNode = consistentHash.get(key);
        
        // å‰¯æœ¬èŠ‚ç‚¹
        List<CacheNode> replicaNodes = consistentHash.getReplicaNodes(key, replicationFactor.getValue());
        
        List<CacheNode> allNodes = new ArrayList<>();
        allNodes.add(primaryNode);
        allNodes.addAll(replicaNodes);
        
        return allNodes;
    }
    
    // åŠ¨æ€æ·»åŠ ç¼“å­˜èŠ‚ç‚¹
    public void addCacheNode(CacheNode newNode) {
        synchronized (this) {
            cacheNodes.add(newNode);
            consistentHash.add(newNode);
            
            // æ•°æ®è¿ç§»
            migrateDataToNewNode(newNode);
            
            log.info("Added new cache node: {}", newNode);
        }
    }
    
    // ç§»é™¤ç¼“å­˜èŠ‚ç‚¹
    public void removeCacheNode(CacheNode nodeToRemove) {
        synchronized (this) {
            // æ•°æ®è¿ç§»åˆ°å…¶ä»–èŠ‚ç‚¹
            migrateDataFromRemovedNode(nodeToRemove);
            
            cacheNodes.remove(nodeToRemove);
            consistentHash.remove(nodeToRemove);
            
            log.info("Removed cache node: {}", nodeToRemove);
        }
    }
    
    private void migrateDataToNewNode(CacheNode newNode) {
        // å¼‚æ­¥æ•°æ®è¿ç§»
        CompletableFuture.runAsync(() -> {
            try {
                Set<String> keysToMigrate = findKeysToMigrate(newNode);
                
                for (String key : keysToMigrate) {
                    migrateKey(key, newNode);
                }
                
                log.info("Data migration completed for node: {}", newNode);
                
            } catch (Exception e) {
                log.error("Data migration failed for node: " + newNode, e);
            }
        });
    }
    
    private void migrateKey(String key, CacheNode targetNode) {
        try {
            // ä»æºèŠ‚ç‚¹è·å–æ•°æ®
            Object value = getFromSourceNode(key);
            
            if (value != null) {
                // å†™å…¥ç›®æ ‡èŠ‚ç‚¹
                writeToTargetNode(targetNode, key, value);
                
                // éªŒè¯æ•°æ®ä¸€è‡´æ€§
                if (verifyMigration(key, targetNode, value)) {
                    // ä»æºèŠ‚ç‚¹åˆ é™¤ï¼ˆå¯é€‰ï¼ŒåŸºäºç­–ç•¥ï¼‰
                    deleteFromSourceNode(key);
                }
            }
            
        } catch (Exception e) {
            log.warn("Failed to migrate key: " + key, e);
        }
    }
}

// ç¼“å­˜é¢„çƒ­å’Œæ·˜æ±°ç­–ç•¥
@Component
public class CacheWarmupAndEvictionService {
    
    private final RedisTemplate<String, Object> redisTemplate;
    private final ScheduledExecutorService warmupExecutor = Executors.newScheduledThreadPool(5);
    
    // æ™ºèƒ½ç¼“å­˜é¢„çƒ­
    @PostConstruct
    public void scheduleWarmupTasks() {
        // æ¯å¤©å‡Œæ™¨2ç‚¹æ‰§è¡Œå…¨é‡é¢„çƒ­
        warmupExecutor.scheduleAtFixedRate(this::performFullWarmup, 
                calculateInitialDelay(2, 0), 
                Duration.ofDays(1).toSeconds(), 
                TimeUnit.SECONDS);
        
        // æ¯å°æ—¶æ‰§è¡Œå¢é‡é¢„çƒ­
        warmupExecutor.scheduleAtFixedRate(this::performIncrementalWarmup, 
                0, 3600, TimeUnit.SECONDS);
    }
    
    public void performFullWarmup() {
        log.info("Starting full cache warmup");
        
        try {
            // é¢„çƒ­ç”¨æˆ·æ•°æ®
            warmupUserData();
            
            // é¢„çƒ­é…ç½®æ•°æ®
            warmupConfigData();
            
            // é¢„çƒ­çƒ­ç‚¹æ•°æ®
            warmupHotData();
            
            log.info("Full cache warmup completed");
            
        } catch (Exception e) {
            log.error("Full cache warmup failed", e);
        }
    }
    
    public void performIncrementalWarmup() {
        try {
            // åŸºäºè®¿é—®æ¨¡å¼é¢„çƒ­
            List<String> hotKeys = getHotKeysFromAccessLog();
            
            for (String key : hotKeys) {
                if (!redisTemplate.hasKey(key)) {
                    Object value = loadFromDatabase(key);
                    if (value != null) {
                        redisTemplate.opsForValue().set(key, value, Duration.ofHours(2));
                    }
                }
            }
            
        } catch (Exception e) {
            log.warn("Incremental cache warmup failed", e);
        }
    }
    
    // æ™ºèƒ½æ·˜æ±°ç­–ç•¥
    public void performIntelligentEviction() {
        MemoryInfo memoryInfo = getRedisMemoryInfo();
        
        if (memoryInfo.getUsagePercentage() > 80) {
            // å†…å­˜ä½¿ç”¨ç‡è¶…è¿‡80%ï¼Œå¼€å§‹æ·˜æ±°
            List<String> candidateKeys = findEvictionCandidates();
            
            // æŒ‰ä¼˜å…ˆçº§æ·˜æ±°
            candidateKeys.sort((k1, k2) -> {
                EvictionScore score1 = calculateEvictionScore(k1);
                EvictionScore score2 = calculateEvictionScore(k2);
                return Double.compare(score1.getScore(), score2.getScore());
            });
            
            int targetEvictionCount = (int) (candidateKeys.size() * 0.1); // æ·˜æ±°10%
            
            for (int i = 0; i < targetEvictionCount && i < candidateKeys.size(); i++) {
                String key = candidateKeys.get(i);
                redisTemplate.delete(key);
                
                log.debug("Evicted cache key: {}", key);
            }
            
            log.info("Evicted {} cache entries to free memory", targetEvictionCount);
        }
    }
    
    private EvictionScore calculateEvictionScore(String key) {
        // ç»¼åˆè€ƒè™‘å¤šä¸ªå› ç´ è®¡ç®—æ·˜æ±°åˆ†æ•°
        long ttl = redisTemplate.getExpire(key, TimeUnit.SECONDS);
        long accessCount = getAccessCount(key);
        long lastAccessTime = getLastAccessTime(key);
        long keySize = getKeySize(key);
        
        // TTLæƒé‡ï¼šTTLè¶ŠçŸ­ï¼Œè¶Šå®¹æ˜“è¢«æ·˜æ±°
        double ttlScore = ttl > 0 ? 1.0 / ttl : 0;
        
        // è®¿é—®é¢‘ç‡æƒé‡ï¼šè®¿é—®æ¬¡æ•°è¶Šå°‘ï¼Œè¶Šå®¹æ˜“è¢«æ·˜æ±°
        double accessScore = 1.0 / Math.max(1, accessCount);
        
        // æœ€è¿‘è®¿é—®æƒé‡ï¼šæœ€è¿‘è®¿é—®æ—¶é—´è¶Šè¿œï¼Œè¶Šå®¹æ˜“è¢«æ·˜æ±°
        double recentScore = 1.0 / Math.max(1, System.currentTimeMillis() - lastAccessTime);
        
        // å¤§å°æƒé‡ï¼šè¶Šå¤§çš„keyè¶Šå®¹æ˜“è¢«æ·˜æ±°
        double sizeScore = Math.log(Math.max(1, keySize));
        
        double totalScore = ttlScore * 0.3 + accessScore * 0.3 + recentScore * 0.3 + sizeScore * 0.1;
        
        return new EvictionScore(key, totalScore);
    }
}
```

## âš–ï¸ è‡ªåŠ¨æ‰©ç¼©å®¹æœºåˆ¶

### åŸºäºæŒ‡æ ‡çš„æ‰©ç¼©å®¹
```yaml
# HPAé…ç½® - åŸºäºå¤šæŒ‡æ ‡
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: wework-api-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: wework-api
  minReplicas: 3
  maxReplicas: 100
  metrics:
  # CPUæŒ‡æ ‡
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  # å†…å­˜æŒ‡æ ‡
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80
  # è‡ªå®šä¹‰æŒ‡æ ‡ - QPS
  - type: Pods
    pods:
      metric:
        name: requests_per_second
      target:
        type: AverageValue
        averageValue: "100"
  # è‡ªå®šä¹‰æŒ‡æ ‡ - å“åº”æ—¶é—´
  - type: Pods
    pods:
      metric:
        name: average_response_time
      target:
        type: AverageValue
        averageValue: "200m"
  # è‡ªå®šä¹‰æŒ‡æ ‡ - é˜Ÿåˆ—é•¿åº¦
  - type: Pods
    pods:
      metric:
        name: queue_length
      target:
        type: AverageValue
        averageValue: "50"
  behavior:
    scaleDown:
      stabilizationWindowSeconds: 300
      policies:
      - type: Percent
        value: 10
        periodSeconds: 60
      - type: Pods
        value: 5
        periodSeconds: 60
    scaleUp:
      stabilizationWindowSeconds: 60
      policies:
      - type: Percent
        value: 50
        periodSeconds: 30
      - type: Pods
        value: 10
        periodSeconds: 30

---
# VPAé…ç½® - å‚ç›´æ‰©ç¼©å®¹
apiVersion: autoscaling.k8s.io/v1
kind: VerticalPodAutoscaler
metadata:
  name: wework-api-vpa
spec:
  targetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: wework-api
  updatePolicy:
    updateMode: "Auto"
  resourcePolicy:
    containerPolicies:
    - containerName: wework-api
      minAllowed:
        cpu: 100m
        memory: 128Mi
      maxAllowed:
        cpu: 4
        memory: 8Gi
      controlledResources: ["cpu", "memory"]
      controlledValues: RequestsAndLimits
```

### é¢„æµ‹æ€§æ‰©å®¹
```java
@Component
public class PredictiveScalingService {
    
    private final MetricsCollector metricsCollector;
    private final MachineLearningPredictor predictor;
    private final AutoScalingController autoScalingController;
    
    // è´Ÿè½½é¢„æµ‹æ¨¡å‹
    @Scheduled(fixedRate = 300000) // æ¯5åˆ†é’Ÿæ‰§è¡Œä¸€æ¬¡
    public void predictAndScale() {
        try {
            // æ”¶é›†å†å²æŒ‡æ ‡æ•°æ®
            List<MetricData> historicalData = metricsCollector.getHistoricalData(
                Duration.ofDays(7), // è¿‡å»7å¤©çš„æ•°æ®
                Duration.ofMinutes(5) // 5åˆ†é’Ÿé—´éš”
            );
            
            // é¢„æµ‹æœªæ¥1å°æ—¶çš„è´Ÿè½½
            LoadPrediction prediction = predictor.predictLoad(historicalData, Duration.ofHours(1));
            
            // åŸºäºé¢„æµ‹ç»“æœå†³å®šæ˜¯å¦éœ€è¦é¢„æ‰©å®¹
            ScalingDecision decision = makeScalingDecision(prediction);
            
            if (decision.shouldScale()) {
                executePreemptiveScaling(decision);
            }
            
        } catch (Exception e) {
            log.error("Predictive scaling failed", e);
        }
    }
    
    private ScalingDecision makeScalingDecision(LoadPrediction prediction) {
        ScalingDecision.Builder builder = ScalingDecision.builder();
        
        // è·å–å½“å‰èµ„æºçŠ¶æ€
        ResourceStatus currentStatus = getCurrentResourceStatus();
        
        // é¢„æµ‹è´Ÿè½½åˆ†æ
        double predictedCpuUsage = prediction.getCpuUsage();
        double predictedMemoryUsage = prediction.getMemoryUsage();
        double predictedQps = prediction.getQps();
        
        // å½“å‰å®¹é‡
        int currentReplicas = currentStatus.getCurrentReplicas();
        double currentCapacity = currentStatus.getCurrentCapacity();
        
        // è®¡ç®—æ‰€éœ€å®¹é‡
        double requiredCapacity = calculateRequiredCapacity(prediction);
        
        if (requiredCapacity > currentCapacity * 1.2) {
            // éœ€è¦æ‰©å®¹
            int targetReplicas = calculateTargetReplicas(requiredCapacity, currentCapacity, currentReplicas);
            builder.scaleUp(targetReplicas - currentReplicas)
                   .reason("Predicted load increase: " + prediction.getSummary());
        } else if (requiredCapacity < currentCapacity * 0.5) {
            // å¯ä»¥ç¼©å®¹
            int targetReplicas = calculateTargetReplicas(requiredCapacity, currentCapacity, currentReplicas);
            builder.scaleDown(currentReplicas - targetReplicas)
                   .reason("Predicted load decrease: " + prediction.getSummary());
        } else {
            builder.noAction("Load prediction within acceptable range");
        }
        
        return builder.build();
    }
    
    private void executePreemptiveScaling(ScalingDecision decision) {
        log.info("Executing preemptive scaling: {}", decision);
        
        if (decision.isScaleUp()) {
            // é¢„æ‰©å®¹
            autoScalingController.scaleUp(decision.getScaleAmount(), ScalingReason.PREDICTIVE);
            
            // è®°å½•æ‰©å®¹äº‹ä»¶
            recordScalingEvent(ScalingType.SCALE_UP, decision);
            
        } else if (decision.isScaleDown()) {
            // é¢„ç¼©å®¹ï¼ˆæ›´ä¿å®ˆï¼‰
            if (decision.getConfidence() > 0.8) { // é«˜ç½®ä¿¡åº¦æ‰ç¼©å®¹
                autoScalingController.scaleDown(decision.getScaleAmount(), ScalingReason.PREDICTIVE);
                recordScalingEvent(ScalingType.SCALE_DOWN, decision);
            }
        }
    }
}

// æœºå™¨å­¦ä¹ é¢„æµ‹å™¨
@Component
public class MachineLearningPredictor {
    
    private final TimeSeriesModel timeSeriesModel;
    private final SeasonalityDetector seasonalityDetector;
    private final AnomalyDetector anomalyDetector;
    
    public LoadPrediction predictLoad(List<MetricData> historicalData, Duration predictionHorizon) {
        // æ•°æ®é¢„å¤„ç†
        List<MetricData> cleanedData = preprocessData(historicalData);
        
        // æ£€æµ‹å­£èŠ‚æ€§æ¨¡å¼
        SeasonalityPattern pattern = seasonalityDetector.detect(cleanedData);
        
        // å¼‚å¸¸æ£€æµ‹å’Œæ•°æ®æ¸…æ´—
        List<MetricData> normalizedData = anomalyDetector.removeAnomalies(cleanedData);
        
        // æ—¶é—´åºåˆ—é¢„æµ‹
        TimeSeriesPrediction tsPrediction = timeSeriesModel.predict(normalizedData, predictionHorizon);
        
        // åº”ç”¨å­£èŠ‚æ€§è°ƒæ•´
        TimeSeriesPrediction adjustedPrediction = applySeasonalityAdjustment(tsPrediction, pattern);
        
        // æ„å»ºè´Ÿè½½é¢„æµ‹ç»“æœ
        return LoadPrediction.builder()
            .cpuUsage(adjustedPrediction.getCpuUsage())
            .memoryUsage(adjustedPrediction.getMemoryUsage())
            .qps(adjustedPrediction.getQps())
            .responseTime(adjustedPrediction.getResponseTime())
            .confidence(adjustedPrediction.getConfidence())
            .predictionHorizon(predictionHorizon)
            .seasonalityPattern(pattern)
            .build();
    }
    
    private List<MetricData> preprocessData(List<MetricData> rawData) {
        return rawData.stream()
            .filter(this::isValidData)
            .map(this::normalizeData)
            .collect(Collectors.toList());
    }
    
    private boolean isValidData(MetricData data) {
        // æ•°æ®æœ‰æ•ˆæ€§æ£€æŸ¥
        return data.getCpuUsage() >= 0 && data.getCpuUsage() <= 1.0 &&
               data.getMemoryUsage() >= 0 && data.getMemoryUsage() <= 1.0 &&
               data.getQps() >= 0 && data.getResponseTime() > 0;
    }
    
    private MetricData normalizeData(MetricData data) {
        // æ•°æ®æ ‡å‡†åŒ–å¤„ç†
        return MetricData.builder()
            .timestamp(data.getTimestamp())
            .cpuUsage(Math.min(1.0, Math.max(0.0, data.getCpuUsage())))
            .memoryUsage(Math.min(1.0, Math.max(0.0, data.getMemoryUsage())))
            .qps(Math.max(0, data.getQps()))
            .responseTime(Math.max(1, data.getResponseTime()))
            .build();
    }
}
```

### æˆæœ¬ä¼˜åŒ–æ‰©ç¼©å®¹
```java
@Component
public class CostOptimizedScalingService {
    
    private final CloudProviderApi cloudApi;
    private final PricingService pricingService;
    private final ResourceOptimizer resourceOptimizer;
    
    // æˆæœ¬æ„ŸçŸ¥çš„æ‰©ç¼©å®¹å†³ç­–
    public ScalingDecision makeCostOptimizedDecision(LoadPrediction prediction) {
        // è·å–å½“å‰èµ„æºæˆæœ¬
        ResourceCost currentCost = calculateCurrentResourceCost();
        
        // è·å–ä¸åŒå®ä¾‹ç±»å‹çš„å®šä»·
        List<InstanceOption> availableInstances = getAvailableInstanceOptions();
        
        // è®¡ç®—å„ç§æ‰©ç¼©å®¹æ–¹æ¡ˆçš„æˆæœ¬æ•ˆç›Š
        List<ScalingOption> scalingOptions = generateScalingOptions(prediction, availableInstances);
        
        // é€‰æ‹©æˆæœ¬æ•ˆç›Šæœ€ä½³çš„æ–¹æ¡ˆ
        ScalingOption bestOption = selectBestOption(scalingOptions, prediction);
        
        return ScalingDecision.builder()
            .scalingOption(bestOption)
            .costSavings(currentCost.subtract(bestOption.getCost()))
            .performanceImpact(bestOption.getPerformanceImpact())
            .build();
    }
    
    private List<ScalingOption> generateScalingOptions(LoadPrediction prediction, 
                                                      List<InstanceOption> instances) {
        List<ScalingOption> options = new ArrayList<>();
        
        for (InstanceOption instance : instances) {
            // è®¡ç®—éœ€è¦å¤šå°‘ä¸ªå®ä¾‹
            int requiredInstances = calculateRequiredInstances(prediction, instance);
            
            // è®¡ç®—æˆæœ¬
            BigDecimal cost = pricingService.calculateCost(instance, requiredInstances, 
                                                          prediction.getDuration());
            
            // è¯„ä¼°æ€§èƒ½å½±å“
            PerformanceImpact performance = evaluatePerformanceImpact(instance, requiredInstances);
            
            // æ£€æŸ¥spotå®ä¾‹å¯ç”¨æ€§
            if (instance.supportsSpotInstance()) {
                SpotInstanceOption spotOption = createSpotInstanceOption(instance, requiredInstances);
                if (spotOption.isAvailable()) {
                    options.add(ScalingOption.builder()
                        .instanceType(instance.getType())
                        .instanceCount(requiredInstances)
                        .cost(spotOption.getCost())
                        .performanceImpact(performance)
                        .riskLevel(RiskLevel.MEDIUM) // Spot instances have medium risk
                        .build());
                }
            }
            
            // æŒ‰éœ€å®ä¾‹é€‰é¡¹
            options.add(ScalingOption.builder()
                .instanceType(instance.getType())
                .instanceCount(requiredInstances)
                .cost(cost)
                .performanceImpact(performance)
                .riskLevel(RiskLevel.LOW)
                .build());
        }
        
        return options;
    }
    
    private ScalingOption selectBestOption(List<ScalingOption> options, LoadPrediction prediction) {
        return options.stream()
            .filter(option -> option.getPerformanceImpact().meetsSLA())
            .min((o1, o2) -> {
                // ç»¼åˆè€ƒè™‘æˆæœ¬ã€æ€§èƒ½å’Œé£é™©
                double score1 = calculateOptionScore(o1, prediction);
                double score2 = calculateOptionScore(o2, prediction);
                return Double.compare(score1, score2);
            })
            .orElse(options.get(0)); // å¦‚æœæ²¡æœ‰æ»¡è¶³SLAçš„é€‰é¡¹ï¼Œé€‰æ‹©ç¬¬ä¸€ä¸ª
    }
    
    private double calculateOptionScore(ScalingOption option, LoadPrediction prediction) {
        // æˆæœ¬æƒé‡
        double costScore = option.getCost().doubleValue() / getMaxAcceptableCost().doubleValue();
        
        // æ€§èƒ½æƒé‡
        double performanceScore = 1.0 - option.getPerformanceImpact().getQualityScore();
        
        // é£é™©æƒé‡
        double riskScore = option.getRiskLevel().getScore();
        
        // åŠ æƒè®¡ç®—æ€»åˆ†ï¼ˆåˆ†æ•°è¶Šä½è¶Šå¥½ï¼‰
        return costScore * 0.5 + performanceScore * 0.3 + riskScore * 0.2;
    }
    
    // æ™ºèƒ½å®ä¾‹ç±»å‹æ¨è
    public InstanceRecommendation recommendInstanceType(WorkloadProfile workload) {
        // åˆ†æå·¥ä½œè´Ÿè½½ç‰¹å¾
        WorkloadCharacteristics characteristics = analyzeWorkload(workload);
        
        List<InstanceOption> candidates = new ArrayList<>();
        
        if (characteristics.isCpuIntensive()) {
            candidates.addAll(getComputeOptimizedInstances());
        }
        
        if (characteristics.isMemoryIntensive()) {
            candidates.addAll(getMemoryOptimizedInstances());
        }
        
        if (characteristics.isIoIntensive()) {
            candidates.addAll(getStorageOptimizedInstances());
        }
        
        if (characteristics.isBalanced()) {
            candidates.addAll(getGeneralPurposeInstances());
        }
        
        // åŸºäºå†å²æ€§èƒ½æ•°æ®è¯„ä¼°
        return evaluateInstancePerformance(candidates, workload);
    }
}
```

## ğŸŒ è·¨åœ°åŸŸéƒ¨ç½²æ‰©å±•

### å¤šåœ°åŸŸæ¶æ„
```yaml
# å¤šåœ°åŸŸéƒ¨ç½²é…ç½®
apiVersion: v1
kind: ConfigMap
metadata:
  name: multi-region-config
data:
  regions.yaml: |
    regions:
      - name: "us-east-1"
        primary: true
        endpoints:
          - "https://api-us-east.wework.com"
        database:
          read_replicas: 2
          write_instance: "wework-db-us-east-master"
        cache:
          cluster_size: 6
          replication_factor: 2
      
      - name: "eu-west-1"
        primary: false
        endpoints:
          - "https://api-eu-west.wework.com"
        database:
          read_replicas: 2
          write_instance: "wework-db-eu-west-master"
        cache:
          cluster_size: 4
          replication_factor: 2
      
      - name: "ap-southeast-1"
        primary: false
        endpoints:
          - "https://api-ap-southeast.wework.com"
        database:
          read_replicas: 2
          write_instance: "wework-db-ap-southeast-master"
        cache:
          cluster_size: 4
          replication_factor: 2
    
    routing:
      strategy: "latency_based"
      fallback_region: "us-east-1"
      health_check_interval: "30s"
      
    data_sync:
      strategy: "eventually_consistent"
      sync_interval: "5m"
      conflict_resolution: "timestamp_based"

---
# å…¨å±€è´Ÿè½½å‡è¡¡é…ç½®
apiVersion: networking.istio.io/v1beta1
kind: Gateway
metadata:
  name: global-gateway
spec:
  selector:
    istio: ingressgateway
  servers:
  - port:
      number: 443
      name: https
      protocol: HTTPS
    tls:
      mode: SIMPLE
      credentialName: wework-tls-cert
    hosts:
    - "*.wework.com"

---
apiVersion: networking.istio.io/v1beta1
kind: VirtualService
metadata:
  name: global-routing
spec:
  hosts:
  - "*.wework.com"
  gateways:
  - global-gateway
  http:
  # åŸºäºåœ°ç†ä½ç½®çš„è·¯ç”±
  - match:
    - headers:
        x-user-region:
          exact: "us"
    route:
    - destination:
        host: wework-api-us-east
      weight: 100
  - match:
    - headers:
        x-user-region:
          exact: "eu"
    route:
    - destination:
        host: wework-api-eu-west
      weight: 100
  - match:
    - headers:
        x-user-region:
          exact: "asia"
    route:
    - destination:
        host: wework-api-ap-southeast
      weight: 100
  # é»˜è®¤è·¯ç”±åˆ°æœ€è¿‘çš„åŒºåŸŸ
  - route:
    - destination:
        host: wework-api-us-east
      weight: 50
    - destination:
        host: wework-api-eu-west
      weight: 30
    - destination:
        host: wework-api-ap-southeast
      weight: 20
```

### è·¨åœ°åŸŸæ•°æ®åŒæ­¥
```java
@Component
public class CrossRegionDataSyncService {
    
    private final Map<String, RegionSyncClient> regionClients;
    private final ConflictResolutionStrategy conflictResolver;
    private final EventualConsistencyManager consistencyManager;
    
    // æ•°æ®åŒæ­¥ç­–ç•¥
    @Scheduled(fixedRate = 300000) // æ¯5åˆ†é’ŸåŒæ­¥ä¸€æ¬¡
    public void synchronizeData() {
        List<SyncTask> syncTasks = createSyncTasks();
        
        // å¹¶è¡Œæ‰§è¡ŒåŒæ­¥ä»»åŠ¡
        syncTasks.parallelStream().forEach(this::executeSyncTask);
    }
    
    private List<SyncTask> createSyncTasks() {
        List<SyncTask> tasks = new ArrayList<>();
        
        // è·å–éœ€è¦åŒæ­¥çš„æ•°æ®
        List<DataChangeRecord> changes = getDataChangesSinceLastSync();
        
        for (DataChangeRecord change : changes) {
            String sourceRegion = change.getSourceRegion();
            List<String> targetRegions = getTargetRegions(sourceRegion);
            
            for (String targetRegion : targetRegions) {
                SyncTask task = SyncTask.builder()
                    .dataChange(change)
                    .sourceRegion(sourceRegion)
                    .targetRegion(targetRegion)
                    .syncType(determineSyncType(change))
                    .build();
                    
                tasks.add(task);
            }
        }
        
        return tasks;
    }
    
    private void executeSyncTask(SyncTask task) {
        try {
            RegionSyncClient client = regionClients.get(task.getTargetRegion());
            
            switch (task.getSyncType()) {
                case FULL_SYNC:
                    executeFullSync(client, task);
                    break;
                case INCREMENTAL_SYNC:
                    executeIncrementalSync(client, task);
                    break;
                case CONFLICT_RESOLUTION:
                    executeConflictResolution(client, task);
                    break;
            }
            
            // è®°å½•åŒæ­¥æˆåŠŸ
            recordSyncSuccess(task);
            
        } catch (Exception e) {
            log.error("Sync task failed: " + task, e);
            handleSyncFailure(task, e);
        }
    }
    
    private void executeIncrementalSync(RegionSyncClient client, SyncTask task) {
        DataChangeRecord change = task.getDataChange();
        
        // æ£€æŸ¥ç›®æ ‡åŒºåŸŸçš„æ•°æ®ç‰ˆæœ¬
        DataVersion targetVersion = client.getDataVersion(change.getEntityId());
        
        if (targetVersion.isOlderThan(change.getVersion())) {
            // ç›®æ ‡æ•°æ®è¾ƒæ—§ï¼Œæ‰§è¡ŒåŒæ­¥
            SyncResult result = client.syncData(change);
            
            if (!result.isSuccess()) {
                if (result.hasConflict()) {
                    // æœ‰å†²çªï¼Œæ‰§è¡Œå†²çªè§£å†³
                    resolveConflict(client, change, result.getConflictData());
                } else {
                    throw new SyncException("Sync failed: " + result.getError());
                }
            }
        } else if (targetVersion.isNewerThan(change.getVersion())) {
            // ç›®æ ‡æ•°æ®è¾ƒæ–°ï¼Œå¯èƒ½éœ€è¦åå‘åŒæ­¥
            handleReverseSync(client, change, targetVersion);
        }
        // ç‰ˆæœ¬ç›¸åŒåˆ™è·³è¿‡
    }
    
    private void resolveConflict(RegionSyncClient client, DataChangeRecord localChange, 
                               ConflictData conflictData) {
        ConflictResolution resolution = conflictResolver.resolve(localChange, conflictData);
        
        switch (resolution.getStrategy()) {
            case LAST_WRITE_WINS:
                applyLastWriteWinsResolution(client, localChange, conflictData);
                break;
            case MERGE:
                applyMergeResolution(client, localChange, conflictData);
                break;
            case MANUAL_RESOLUTION:
                scheduleManualResolution(client, localChange, conflictData);
                break;
        }
    }
    
    // æœ€ç»ˆä¸€è‡´æ€§ç®¡ç†
    @Component
    public static class EventualConsistencyManager {
        
        private final ConsistencyCheckScheduler scheduler;
        private final InconsistencyDetector detector;
        
        @Scheduled(fixedRate = 1800000) // æ¯30åˆ†é’Ÿæ£€æŸ¥ä¸€æ¬¡
        public void checkConsistency() {
            List<ConsistencyCheckTask> checkTasks = createConsistencyCheckTasks();
            
            for (ConsistencyCheckTask task : checkTasks) {
                CompletableFuture.runAsync(() -> executeConsistencyCheck(task));
            }
        }
        
        private void executeConsistencyCheck(ConsistencyCheckTask task) {
            try {
                List<DataInconsistency> inconsistencies = detector.detectInconsistencies(task);
                
                if (!inconsistencies.isEmpty()) {
                    log.warn("Found {} inconsistencies in {}", 
                            inconsistencies.size(), task.getEntityType());
                    
                    scheduleInconsistencyResolution(inconsistencies);
                }
                
            } catch (Exception e) {
                log.error("Consistency check failed for task: " + task, e);
            }
        }
        
        private void scheduleInconsistencyResolution(List<DataInconsistency> inconsistencies) {
            for (DataInconsistency inconsistency : inconsistencies) {
                InconsistencyResolutionTask resolutionTask = InconsistencyResolutionTask.builder()
                    .inconsistency(inconsistency)
                    .priority(calculatePriority(inconsistency))
                    .scheduledAt(LocalDateTime.now())
                    .build();
                
                inconsistencyResolutionQueue.offer(resolutionTask);
            }
        }
    }
}
```

## ğŸ“Š ç›‘æ§å’Œå®¹é‡è§„åˆ’

### æ‰©å±•æ€§ç›‘æ§æŒ‡æ ‡
```java
@Component
public class ScalabilityMetricsCollector {
    
    private final MeterRegistry meterRegistry;
    private final InfluxDBClient influxDBClient;
    
    // å®æ—¶æŒ‡æ ‡æ”¶é›†
    @EventListener
    public void collectResourceMetrics(ResourceMetricsEvent event) {
        // CPUæŒ‡æ ‡
        Gauge.builder("system.cpu.usage")
            .tag("region", event.getRegion())
            .tag("instance", event.getInstanceId())
            .register(meterRegistry, event, ResourceMetricsEvent::getCpuUsage);
        
        // å†…å­˜æŒ‡æ ‡
        Gauge.builder("system.memory.usage")
            .tag("region", event.getRegion())
            .tag("instance", event.getInstanceId())
            .register(meterRegistry, event, ResourceMetricsEvent::getMemoryUsage);
        
        // ç½‘ç»œæŒ‡æ ‡
        meterRegistry.counter("system.network.bytes.in", 
            "region", event.getRegion(),
            "instance", event.getInstanceId())
            .increment(event.getNetworkBytesIn());
        
        meterRegistry.counter("system.network.bytes.out",
            "region", event.getRegion(), 
            "instance", event.getInstanceId())
            .increment(event.getNetworkBytesOut());
    }
    
    @EventListener
    public void collectApplicationMetrics(ApplicationMetricsEvent event) {
        // QPSæŒ‡æ ‡
        meterRegistry.counter("application.requests.total",
            "service", event.getServiceName(),
            "region", event.getRegion(),
            "status", event.getStatus())
            .increment(event.getRequestCount());
        
        // å“åº”æ—¶é—´æŒ‡æ ‡
        Timer.Sample sample = Timer.start(meterRegistry);
        sample.stop(Timer.builder("application.response.time")
            .tag("service", event.getServiceName())
            .tag("region", event.getRegion())
            .tag("endpoint", event.getEndpoint())
            .register(meterRegistry));
        
        // é”™è¯¯ç‡æŒ‡æ ‡
        if (event.hasError()) {
            meterRegistry.counter("application.errors.total",
                "service", event.getServiceName(),
                "region", event.getRegion(),
                "error_type", event.getErrorType())
                .increment();
        }
    }
    
    // å®¹é‡æŒ‡æ ‡è®¡ç®—
    @Scheduled(fixedRate = 60000) // æ¯åˆ†é’Ÿè®¡ç®—ä¸€æ¬¡
    public void calculateCapacityMetrics() {
        List<String> regions = getActiveRegions();
        
        for (String region : regions) {
            RegionCapacityMetrics capacity = calculateRegionCapacity(region);
            
            // å½“å‰å®¹é‡åˆ©ç”¨ç‡
            meterRegistry.gauge("capacity.utilization.cpu", 
                Tags.of("region", region), capacity.getCpuUtilization());
            
            meterRegistry.gauge("capacity.utilization.memory", 
                Tags.of("region", region), capacity.getMemoryUtilization());
            
            meterRegistry.gauge("capacity.utilization.network", 
                Tags.of("region", region), capacity.getNetworkUtilization());
            
            // å‰©ä½™å®¹é‡
            meterRegistry.gauge("capacity.remaining.cpu", 
                Tags.of("region", region), capacity.getRemainingCpuCapacity());
            
            meterRegistry.gauge("capacity.remaining.memory", 
                Tags.of("region", region), capacity.getRemainingMemoryCapacity());
            
            // å®¹é‡è­¦å‘Š
            if (capacity.getCpuUtilization() > 0.8) {
                alertService.sendCapacityAlert(region, "CPU", capacity.getCpuUtilization());
            }
            
            if (capacity.getMemoryUtilization() > 0.8) {
                alertService.sendCapacityAlert(region, "Memory", capacity.getMemoryUtilization());
            }
        }
    }
    
    // æ‰©å±•æ€§ç“¶é¢ˆåˆ†æ
    public BottleneckAnalysisReport analyzeBottlenecks(String region, Duration analysisWindow) {
        LocalDateTime endTime = LocalDateTime.now();
        LocalDateTime startTime = endTime.minus(analysisWindow);
        
        // æ”¶é›†åˆ†æçª—å£å†…çš„æŒ‡æ ‡æ•°æ®
        List<MetricDataPoint> cpuData = getMetricData("system.cpu.usage", region, startTime, endTime);
        List<MetricDataPoint> memoryData = getMetricData("system.memory.usage", region, startTime, endTime);
        List<MetricDataPoint> networkData = getMetricData("system.network.throughput", region, startTime, endTime);
        List<MetricDataPoint> responseTimeData = getMetricData("application.response.time", region, startTime, endTime);
        
        // ç“¶é¢ˆæ£€æµ‹
        List<Bottleneck> bottlenecks = new ArrayList<>();
        
        // CPUç“¶é¢ˆ
        if (hasBottleneck(cpuData, 0.9)) {
            bottlenecks.add(Bottleneck.builder()
                .type(BottleneckType.CPU)
                .severity(calculateSeverity(cpuData))
                .duration(calculateBottleneckDuration(cpuData, 0.9))
                .impact("High CPU usage may cause request delays")
                .recommendation("Consider adding more CPU-optimized instances")
                .build());
        }
        
        // å†…å­˜ç“¶é¢ˆ
        if (hasBottleneck(memoryData, 0.85)) {
            bottlenecks.add(Bottleneck.builder()
                .type(BottleneckType.MEMORY)
                .severity(calculateSeverity(memoryData))
                .duration(calculateBottleneckDuration(memoryData, 0.85))
                .impact("High memory usage may cause GC pressure")
                .recommendation("Consider adding memory-optimized instances")
                .build());
        }
        
        // ç½‘ç»œç“¶é¢ˆ
        if (hasNetworkBottleneck(networkData)) {
            bottlenecks.add(Bottleneck.builder()
                .type(BottleneckType.NETWORK)
                .severity(BottleneckSeverity.MEDIUM)
                .impact("Network bandwidth limitation")
                .recommendation("Consider network-optimized instances or CDN")
                .build());
        }
        
        // å“åº”æ—¶é—´ç“¶é¢ˆ
        if (hasResponseTimeBottleneck(responseTimeData)) {
            bottlenecks.add(Bottleneck.builder()
                .type(BottleneckType.RESPONSE_TIME)
                .severity(BottleneckSeverity.HIGH)
                .impact("Slow response times affecting user experience")
                .recommendation("Investigate application performance or scale up")
                .build());
        }
        
        return BottleneckAnalysisReport.builder()
            .region(region)
            .analysisWindow(analysisWindow)
            .bottlenecks(bottlenecks)
            .overallHealthScore(calculateOverallHealthScore(bottlenecks))
            .recommendations(generateRecommendations(bottlenecks))
            .build();
    }
}

// å®¹é‡è§„åˆ’æœåŠ¡
@Component
public class CapacityPlanningService {
    
    private final HistoricalDataAnalyzer dataAnalyzer;
    private final GrowthRatePredictor growthPredictor;
    private final CostCalculator costCalculator;
    
    // å®¹é‡è§„åˆ’æŠ¥å‘Š
    public CapacityPlanningReport generatePlanningReport(Duration planningHorizon) {
        // åˆ†æå†å²å¢é•¿è¶‹åŠ¿
        GrowthTrend trend = dataAnalyzer.analyzeGrowthTrend(Duration.ofDays(90));
        
        // é¢„æµ‹æœªæ¥å®¹é‡éœ€æ±‚
        CapacityForecast forecast = growthPredictor.predictCapacityNeeds(trend, planningHorizon);
        
        // è®¡ç®—ä¸åŒæ‰©å±•æ–¹æ¡ˆçš„æˆæœ¬
        List<ExpansionPlan> expansionPlans = generateExpansionPlans(forecast);
        
        // é€‰æ‹©æœ€ä¼˜æ–¹æ¡ˆ
        ExpansionPlan recommendedPlan = selectOptimalPlan(expansionPlans);
        
        return CapacityPlanningReport.builder()
            .planningHorizon(planningHorizon)
            .currentCapacity(getCurrentCapacity())
            .forecastedDemand(forecast)
            .recommendedPlan(recommendedPlan)
            .alternativePlans(expansionPlans)
            .costAnalysis(calculateCostAnalysis(expansionPlans))
            .build();
    }
    
    private List<ExpansionPlan> generateExpansionPlans(CapacityForecast forecast) {
        List<ExpansionPlan> plans = new ArrayList<>();
        
        // æ–¹æ¡ˆ1ï¼šä¿å®ˆæ‰©å±•
        plans.add(ExpansionPlan.builder()
            .name("Conservative Growth")
            .description("Minimal expansion to meet forecasted demand")
            .targetCapacity(forecast.getDemand().multiply(BigDecimal.valueOf(1.1)))
            .expansionStrategy(ExpansionStrategy.GRADUAL)
            .timeline(Timeline.of(Duration.ofMonths(6)))
            .riskLevel(RiskLevel.LOW)
            .build());
        
        // æ–¹æ¡ˆ2ï¼šæ¿€è¿›æ‰©å±•
        plans.add(ExpansionPlan.builder()
            .name("Aggressive Growth")
            .description("Rapid expansion to exceed forecasted demand")
            .targetCapacity(forecast.getDemand().multiply(BigDecimal.valueOf(1.5)))
            .expansionStrategy(ExpansionStrategy.RAPID)
            .timeline(Timeline.of(Duration.ofMonths(3)))
            .riskLevel(RiskLevel.HIGH)
            .build());
        
        // æ–¹æ¡ˆ3ï¼šåˆ†é˜¶æ®µæ‰©å±•
        plans.add(ExpansionPlan.builder()
            .name("Phased Growth")
            .description("Multi-phase expansion aligned with demand growth")
            .targetCapacity(forecast.getDemand().multiply(BigDecimal.valueOf(1.25)))
            .expansionStrategy(ExpansionStrategy.PHASED)
            .timeline(Timeline.of(Duration.ofMonths(9)))
            .riskLevel(RiskLevel.MEDIUM)
            .build());
        
        return plans;
    }
    
    private ExpansionPlan selectOptimalPlan(List<ExpansionPlan> plans) {
        return plans.stream()
            .min((p1, p2) -> {
                // ç»¼åˆè¯„åˆ†ï¼šæˆæœ¬ã€é£é™©ã€æ—¶é—´
                double score1 = calculatePlanScore(p1);
                double score2 = calculatePlanScore(p2);
                return Double.compare(score1, score2);
            })
            .orElse(plans.get(0));
    }
    
    private double calculatePlanScore(ExpansionPlan plan) {
        // æˆæœ¬æƒé‡ (40%)
        double costScore = plan.getCost().doubleValue() / getMaxBudget().doubleValue();
        
        // é£é™©æƒé‡ (30%)
        double riskScore = plan.getRiskLevel().getScore();
        
        // æ—¶é—´æƒé‡ (20%)
        double timeScore = plan.getTimeline().getDuration().toDays() / 365.0;
        
        // å®¹é‡ä½™é‡æƒé‡ (10%)
        double capacityScore = plan.getCapacityBuffer() / getMaxCapacityBuffer();
        
        return costScore * 0.4 + riskScore * 0.3 + timeScore * 0.2 + capacityScore * 0.1;
    }
}
```

## ğŸ¯ æ‰©å±•æ€§è®¾è®¡æ€»ç»“

### æ ¸å¿ƒæ‰©å±•ç­–ç•¥
1. **æ°´å¹³æ‰©å±•**: æ”¯æŒæ— é™åˆ¶çš„èŠ‚ç‚¹æ‰©å±•
2. **æ™ºèƒ½è´Ÿè½½å‡è¡¡**: åŸºäºæ€§èƒ½çš„åŠ¨æ€è·¯ç”±
3. **é¢„æµ‹æ€§æ‰©å®¹**: æœºå™¨å­¦ä¹ é©±åŠ¨çš„å®¹é‡é¢„æµ‹
4. **æˆæœ¬ä¼˜åŒ–**: æ™ºèƒ½å®ä¾‹é€‰æ‹©å’Œèµ„æºä¼˜åŒ–
5. **è·¨åœ°åŸŸéƒ¨ç½²**: å…¨çƒåŒ–æœåŠ¡èƒ½åŠ›
6. **è‡ªåŠ¨æ‰©ç¼©å®¹**: åŸºäºå¤šæŒ‡æ ‡çš„å¼¹æ€§ä¼¸ç¼©

### æ‰©å±•æŒ‡æ ‡
- **æ°´å¹³æ‰©å±•**: æ”¯æŒ1000+èŠ‚ç‚¹çº¿æ€§æ‰©å±•
- **æ‰©å®¹é€Ÿåº¦**: å¹³å‡æ‰©å®¹æ—¶é—´ < 3åˆ†é’Ÿ
- **æˆæœ¬æ•ˆç‡**: è‡ªåŠ¨åŒ–æˆæœ¬ä¼˜åŒ– > 30%
- **å¯ç”¨æ€§**: è·¨åœ°åŸŸå¯ç”¨æ€§ > 99.95%
- **æ€§èƒ½ä¿æŒ**: æ‰©å±•è¿‡ç¨‹ä¸­æ€§èƒ½ä¸é™çº§

### æŠ€æœ¯ç‰¹æ€§
- **å¾®æœåŠ¡æ¶æ„**: æœåŠ¡ç‹¬ç«‹æ‰©å±•å’Œéƒ¨ç½²
- **å®¹å™¨ç¼–æ’**: Kubernetesè‡ªåŠ¨åŒ–ç®¡ç†
- **æœåŠ¡ç½‘æ ¼**: Istioæµé‡ç®¡ç†å’Œå®‰å…¨
- **å¤šäº‘éƒ¨ç½²**: é¿å…å‚å•†é”å®š
- **æ™ºèƒ½ç›‘æ§**: å®æ—¶å®¹é‡å’Œæ€§èƒ½ç›‘æ§
- **é¢„æµ‹åˆ†æ**: AIé©±åŠ¨çš„å®¹é‡è§„åˆ’
